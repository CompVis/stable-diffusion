# AltCLIP for Huggingface

æˆ‘ä»¬å·²ç»ä¸Šä¼ äº†æ¨¡åž‹æƒé‡åˆ° `transformers` ï¼Œåªéœ€è¦å‡ è¡Œä»£ç å°±èƒ½å¿«é€Ÿä½¿ç”¨æˆ‘ä»¬çš„æ¨¡åž‹ï¼ [Huggingface Model Card](https://huggingface.co/BAAI/AltCLIP)

we have uploaded our model to `transformers`. you can use our model by a few lines of code. If you find it useful, feel free to starðŸŒŸ!


# requirements

æˆ‘ä»¬åœ¨ä»¥ä¸‹çŽ¯å¢ƒè¿›è¡Œäº†æµ‹è¯•ï¼Œè¯·å°½é‡ä¿è¯åŒ…ç‰ˆæœ¬ç¬¦åˆè¦æ±‚ã€‚

```
transformeres >= 4.21.0
```
# Inference Code

```python

from PIL import Image
import requests

# transformers version >= 4.21.0
from modeling_altclip import AltCLIP
from processing_altclip import AltCLIPProcessor

# now our repo's in private, so we need `use_auth_token=True`
model = AltCLIP.from_pretrained("BAAI/AltCLIP")
processor = AltCLIPProcessor.from_pretrained("BAAI/AltCLIP")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities

```